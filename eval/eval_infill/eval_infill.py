import torch
import json
import os
import datetime
from typing import List, Dict, Any
from tqdm import tqdm
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import AutoTokenizer
from datasets import load_dataset
from veomni.models.transformers.qwen2.modeling_qwen2 import Qwen2ForCausalLM
from veomni.models.transformers.qwen2.generation_utils import (
    MDMGenerationConfig
)
from utils import upload_to_wandb, run_evaluation_tool


def infilling(
    model,
    tokenizer,
    prompts: List[str],
    middle_lens: List[int],
    suffixs: List[str],
    max_length: int = 2048,
    batch_size: int = 32,
    steps: int = 100,
    temperature: float = 0.5,
    top_p: float = 0.95,
    alg: str = 'p2',
    alg_temp: float = 0.5,
    device: str = 'cuda',
    rank: int = 0
) -> List[str]:
    """Code infilling function similar to generator_temp.py"""
    # Tokenize prompts
    tokenized_prompts = [
        tokenizer.encode(p, add_special_tokens=True) for p in prompts
    ]
    prefix_lens = [len(p) for p in tokenized_prompts]

    # Construct sequences: prefix + masks + suffix
    sequences = [
        p + [tokenizer.mask_token_id] * m +
        tokenizer.encode(s, add_special_tokens=False)
        for p, m, s in zip(tokenized_prompts, middle_lens, suffixs)
    ]
#     import pdb; pdb.set_trace()

    # Truncate if needed
    sequences = [seq[-max_length:] for seq in sequences]

    generations = []
    num_batches = (len(sequences) + batch_size - 1) // batch_size
    
    for i in tqdm(range(0, len(sequences), batch_size), 
                  desc="Generating batches", 
                  total=num_batches,
                  disable=(rank != 0)):
        batch_seqs = sequences[i:i + batch_size]
        batch_prefix_lens = prefix_lens[i:i + batch_size]
        batch_middle_lens = middle_lens[i:i + batch_size]

        # Pad sequences to same length
        max_len = max(len(seq) for seq in batch_seqs)
        padded_seqs = torch.LongTensor([
            seq + [tokenizer.pad_token_id] * (max_len - len(seq))
            for seq in batch_seqs
        ]).to(device)

        # Generate using diffusion
        generation_config = MDMGenerationConfig(
            mask_token_id=tokenizer.mask_token_id,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=10000000,  # No new tokens, just to avoid transformer error
            steps=steps,
            temperature=temperature,
            top_p=top_p,
            alg=alg,
            alg_temp=alg_temp,
            return_dict_in_generate=True
        )

        with torch.no_grad():
            # Handle DDP wrapped model
            actual_model = (
                model.module if hasattr(model, 'module') else model
            )
            outputs = actual_model._mdm_sample(
                x=padded_seqs,
                attention_mask=None,
                generation_config=generation_config
            )

        # Extract middle parts and decode
        batch_results = outputs.sequences
        for j, (result, pl, ml) in enumerate(zip(
            batch_results, batch_prefix_lens, batch_middle_lens
        )):
            middle_part = result[pl:pl + ml]
            decoded = tokenizer.decode(
                middle_part.tolist(), skip_special_tokens=True
            )
            generations.append(decoded)

    return generations


def setup_ddp():
    """Initialize DDP using environment variables set by torchrun"""
    dist.init_process_group("nccl", timeout=datetime.timedelta(seconds=3600))
    rank = int(os.environ['RANK'])
    local_rank = int(os.environ['LOCAL_RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    torch.cuda.set_device(local_rank)
    return rank, local_rank, world_size


def cleanup():
    dist.destroy_process_group()



def eval_infill(
    model_path: str = "fredzzp/open-dcoder-0.5B",
    task: str = 'humaneval_infill',
    prediction_path: str = "infill_results.jsonl",
    fix_middle_length: int = None,
    steps: int = 512,
    temperature: float = 0.2,
    top_p: float = 0.95,
    alg: str = 'p2',
    alg_temp: float = 0.5,
    use_ddp: bool = False,
    auto_eval: bool = True,
    use_wandb: bool = True
):
    """Evaluate infilling performance on humaneval_infill dataset"""
    
    # Extract model name from path
    model_name = model_path.split('/')[-1] if '/' in model_path else model_path
    
    # Create results directory structure: infill_results/task/model_name/temperature
    results_dir = os.path.join("infill_results", task, model_name, str(temperature))
    os.makedirs(results_dir, exist_ok=True)
    
    prediction_path = os.path.join(results_dir, f"{task}_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl")
    
    # Collect configuration parameters for wandb
    config_params = {
        'model_path': model_path,
        'model_name': model_name,
        'task': task,
        'fix_middle_length': fix_middle_length,
        'steps': steps,
        'temperature': temperature,
        'top_p': top_p,
        'alg': alg,
        'alg_temp': alg_temp,
        'use_ddp': use_ddp,
        'auto_eval': auto_eval,
        'results_dir': results_dir
    }
    
    # Initialize DDP if requested
    if use_ddp:
        rank, local_rank, world_size = setup_ddp()
        device = f"cuda:{local_rank}"
        
        # Only print on rank 0 to avoid clutter
        if rank == 0:
            print(f"Initialized DDP: rank {rank}, local_rank {local_rank}, world_size {world_size}")
    else:
        rank, local_rank, world_size = 0, 0, 1
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Load model and tokenizer
    if rank == 0:
        print(f"Loading model: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # Initialize parallel state for custom model if using DDP
    if use_ddp:
        from veomni.distributed.parallel_state import init_parallel_state
        init_parallel_state(
            dp_size=world_size,
            tp_size=1,
            ep_size=1,
            pp_size=1,
            cp_size=1,
            ulysses_size=1,
            dp_mode="ddp",
            device_type="cuda",
            include_sp_in_fsdp=True,
        )
    
    model = Qwen2ForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    if use_ddp:
        model = model.to(local_rank)
        model = DDP(model, device_ids=[local_rank])
    else:
        model = model.to(device)
    
    model.eval()

    # Set mask token if needed
    if tokenizer.mask_token is None:
        raise ValueError("Mask token not found in tokenizer")

    # Load dataset
    if rank == 0:
        print(f"Loading {task} dataset...")
    
    if task == 'humaneval_infill':
        from human_eval_infilling.data import read_problems
        problems = read_problems(benchmark_name='single-line')
#         from itertools import islice
#         problems = dict(islice(problems.items(), 3))
        prefixs = [problems[task_id]["prompt"] for task_id in problems]
        suffixs = [problems[task_id]["suffix"] for task_id in problems]
        ground_truth_middles = [
            problems[task_id]["canonical_solution"] for task_id in problems
        ]
        task_ids = list(problems.keys())
    elif task == 'santacoder-fim':
        # Load SantaCoder FIM dataset
        fim_data = load_dataset("bigcode/santacoder-fim-task", split="train")
        fim_data = [d for d in fim_data if d["language"] == "py"]
        prefixs = [d["prompt"] + '\n' for d in fim_data]
        suffixs = [d["suffix"] for d in fim_data]
        ground_truth_middles = [d["canonical_solution"] for d in fim_data]
        task_ids = [i for i in range(len(fim_data))]
        
        if rank == 0:
            print(f"Loaded {len(fim_data)} examples from santacoder-fim")
            print("Example prefix:", prefixs[0][:100] + "...")
            print("Example suffix:", suffixs[0][:100] + "...")
    else:
        raise ValueError(f"Unsupported task: {task}")

    # Shard dataset by rank for DDP
    if use_ddp:
        total = len(prefixs)
        per_rank = total // world_size
        start = rank * per_rank
        end = start + per_rank if rank < world_size - 1 else total
        
        prefixs_shard = prefixs[start:end]
        suffixs_shard = suffixs[start:end]
        ground_truth_middles_shard = ground_truth_middles[start:end]
        task_ids_shard = task_ids[start:end]
    else:
        prefixs_shard = prefixs
        suffixs_shard = suffixs
        ground_truth_middles_shard = ground_truth_middles
        task_ids_shard = task_ids

    # Calculate middle lengths
    if fix_middle_length:
        middle_lens = [fix_middle_length] * len(task_ids_shard)
    else:
        # Oracle setting: use ground truth lengths
        middle_lens = [
            len(tokenizer.encode(gt, add_special_tokens=False))
            for gt in ground_truth_middles_shard
        ]

    # Print maximum middle_lens
    if rank == 0:
        max_middle_len = max(middle_lens) if middle_lens else 0
        print(f"Maximum middle_lens: {max_middle_len}")
        # import pdb; pdb.set_trace()

    # Generate completions
    if rank == 0:
        print(f"Generating completions for {len(prefixs_shard)} examples...")
    
    generations = infilling(
        model=model,
        tokenizer=tokenizer,
        prompts=prefixs_shard,
        middle_lens=middle_lens,
        suffixs=suffixs_shard,
        steps=steps,
        temperature=temperature,
        top_p=top_p,
        alg=alg,
        alg_temp=alg_temp,
        device=device,
        rank=rank
    )

    # Prepare samples
    samples = [
        {
            'task_id': task_id,
            'completion': pred,
            'ground_truth_middle': ground_truth_middle,
            'prefix': prefix,
            'suffix': suffix
        }
        for task_id, pred, ground_truth_middle, prefix, suffix in zip(
            task_ids_shard, generations, ground_truth_middles_shard,
            prefixs_shard, suffixs_shard
        )
    ]

    if use_ddp:
        # Gather results from all ranks
        gathered_samples = [None] * world_size
        dist.all_gather_object(gathered_samples, samples)
        
        if local_rank == 0:
            merged_samples = []
            for s in gathered_samples:
                merged_samples.extend(s)
            
            print(f"Saving results to {prediction_path}")
            with open(prediction_path, 'w') as f:
                for sample in merged_samples:
                    f.write(json.dumps(sample) + '\n')
            
            print(f"Evaluation complete! Results saved to {prediction_path}")
            
            # Run automatic evaluation if enabled
            eval_results = None
            if auto_eval:
                eval_results = run_evaluation_tool(task, prediction_path, local_rank)
            
            # Save evaluation results if available
            if eval_results:
                eval_file = prediction_path.replace('.jsonl', '_eval_results.json')
                eval_data = {
                    "config": config_params,
                    "results": eval_results
                }
                with open(eval_file, 'w') as f:
                    json.dump(eval_data, f, indent=2)
                print(f"Evaluation results saved to {eval_file}")
            
            # Upload to wandb if enabled
            if use_wandb:
                upload_to_wandb(
                    task=task,
                    model_path=model_path,
                    prediction_path=prediction_path,
                    eval_results=eval_results,
                    config_params=config_params,
                    rank=local_rank
                )
            
        # Cleanup DDP
        cleanup()
        
        # Return both samples and evaluation results
        final_results = {
            'samples': gathered_samples if local_rank == 0 else samples,
            'eval_results': eval_results if local_rank == 0 else None
        }
        return final_results
    else:
        print(f"Saving results to {prediction_path}")
        with open(prediction_path, 'w') as f:
            for sample in samples:
                f.write(json.dumps(sample) + '\n')
        
        print(f"Evaluation complete! Results saved to {prediction_path}")
        
        # Run automatic evaluation if enabled
        eval_results = None
        if auto_eval:
            eval_results = run_evaluation_tool(task, prediction_path, rank)
            
        # Save evaluation results if available
        if eval_results:
            eval_file = prediction_path.replace('.jsonl', '_eval_results.json')
            eval_data = {
                "config": config_params,
                "results": eval_results
            }
            with open(eval_file, 'w') as f:
                json.dump(eval_data, f, indent=2)
            print(f"Evaluation results saved to {eval_file}")
            
        # Upload to wandb if enabled
        if use_wandb:
            upload_to_wandb(
                task=task,
                model_path=model_path,
                prediction_path=prediction_path,
                eval_results=eval_results,
                config_params=config_params,
                rank=rank
            )
            
        # Return both samples and evaluation results
        return {
            'samples': samples,
            'eval_results': eval_results
        }


if __name__ == "__main__":
    # Simple CLI interface
    import argparse
    parser = argparse.ArgumentParser(
        description="Evaluate code infilling on humaneval_infill or santacoder-fim"
    )
    parser.add_argument("--model_path", default="fredzzp/open-dcoder-0.5B",
                        help="Model path or HF model name")
    parser.add_argument("--task", default="humaneval_infill",
                        choices=["humaneval_infill", "santacoder-fim"],
                        help="Evaluation task")
    parser.add_argument("--prediction_path", default="infill_results.jsonl",
                        help="Output file for results")
    parser.add_argument("--fix_middle_length", type=int, default=None,
                        help="Fixed middle length (default: use oracle)")
    parser.add_argument("--steps", type=int, default=64,
                        help="Number of diffusion steps")
    parser.add_argument("--temperature", type=float, default=0.8,
                        help="Sampling temperature")
    parser.add_argument("--top_p", type=float, default=0.95,
                        help="Top-p sampling")
    parser.add_argument("--alg", default="p2",
                        choices=["p2", "origin", "maskgit_plus", "topk_margin", "entropy"],
                        help="Sampling algorithm")
    parser.add_argument("--alg_temp", type=float, default=0.5,
                        help="Algorithm temperature for Gumbel noise")
    parser.add_argument("--use_ddp", action="store_true",
                        help="Enable distributed data parallel")
    parser.add_argument("--no_auto_eval", action="store_true",
                        help="Disable automatic evaluation")
    parser.add_argument("--no_wandb", action="store_true",
                        help="Disable wandb logging")

    args = parser.parse_args()
    
    # Convert no_auto_eval to auto_eval and no_wandb to use_wandb
    args.auto_eval = not args.no_auto_eval
    args.use_wandb = not args.no_wandb
    delattr(args, 'no_auto_eval')
    delattr(args, 'no_wandb')
    
    # Path generation is now handled in eval_infill function
    
    eval_infill(**vars(args))
